import pandas as pd
import pyspark.sql.functions as F
from pyspark import StorageLevel
from pyspark.sql import DataFrame

from tsf import logger
from tsf.bq import get_bq_data
from tsf.data import cumulative_to_daily, zip_results, await_keyboard, read_local_data, number_unique_values, \
    normalize_covid
from tsf.forecast import state_forecast

##################################################################################################
# TODO Edit these according to your project
bq_credentials_file = "./bq_credentials.json"
bq_project_id = 'spark8795'
bq_project_dataset = 'spark8795'
##################################################################################################

forecast_directory = "forecasts"
# BigQuery tables, used in BigQuery load
cases_table = 'bigquery-public-data.covid19_usafacts.confirmed_cases'
deaths_table = 'bigquery-public-data.covid19_usafacts.deaths'


# Ideally aggregate data before join, to reduce join size; however, aggregation is part of the student exercise. :-)
def create_dataset(local_data=True, max_state=None) -> DataFrame:
    """
    Prepare the COVID19 data (bigquery-public-data.covid19_usafacts) for analysis.
    :param local_data: If True, read data from local files.
    :param max_state: If not None, then process states that are alphabetically less than the word in max_state. Used for testing.
    :return: one `DataFrame` with all the data.
    """
    # Read the data files, normalize each data (normalize_covid), and rename each counts
    if local_data:
        logger.info("Getting data from local files.")
        cases = read_local_data("./data/covid/usa_covid_cases.json")
        deaths = read_local_data("./data/covid/usa_covid_deaths.json")
    else:
        logger.info("Getting data from Google BigQuery.")
        cases = get_bq_data(cases_table,
                            bq_credentials_file=bq_credentials_file,
                            project_id=bq_project_id,
                            project_dataset=bq_project_dataset)
        deaths = get_bq_data(deaths_table,
                             bq_credentials_file=bq_credentials_file,
                             project_id=bq_project_id,
                             project_dataset=bq_project_dataset)
    if max_state:
        # Subset states (for testing)
        cases = cases.where(f"state <'{max_state}'")
        deaths = deaths.where(f"state <'{max_state}'")
    logger.debug("Repartitioning data by state.")
    # Smaller partitions are processed by smaller executors, which fit within small student computers.
    cases = cases.repartition(number_unique_values(cases), 'state')
    deaths = deaths.repartition(number_unique_values(deaths), 'state')
    logger.debug("Normalizing data.")
    cases = normalize_covid(cases).withColumnRenamed('counts', 'cases')
    deaths = normalize_covid(deaths).withColumnRenamed('counts', 'deaths')
    # Without cache, join fails on Kubernetes (BUG!?)
    cases = cases.cache()
    deaths = deaths.cache()
    logger.debug("Joining case and death data")
    # Join into one DataFrame
    df = cases.join(deaths, ['state', 'county_fips_code', 'date'])
    # Drop duplicate columns
    df = df.drop(deaths.state_fips_code).drop(deaths.county_name)
    return df


# TODO Write this based on test_state_forecast and the use of applyInPandas with state_forecast_udf
#  schema=("state string, date date, past_mean float, forecast_mean float, mean_change float, forecast_fig string, metrics_fig string"))
def covid_state_predictions(data) -> DataFrame:
    """
    Generates state forecasts of COVID deaths.
    For each state, a figure is saved for the forecast and RMSE metric.
    :return: Summary PySpark DataFrame generated by applying state_forecast_udf to the data
    """
    # HINTS
    # First, groupBy state, agg, sum to create a DataFrame having "state", "date", "daily_deaths as deaths", "daily_cases as cases"
    # Then, return the result of applyInPandas state_forecast_udf to the above DataFrame
    # Like Listing 9.11 Split-apply-combing in PySpark in Rioux
    # https://www.manning.com/books/data-analysis-with-python-and-pyspark
    pass


# TODO Write this summary by using PySpark agg function with avg and max over the covid_state_predictions summary df
def summarize_state_forecasts(df):
    """
    Logs a 1-line summary of the average past and forecast deaths, and average change.
    :param df: covid_state_predictions summary of predictions.
    :return: None
    Example output:
    22/04/24 19:34:53 INFO: tsf: Averages for 51 states with last update at 2022-02-23 are:
    past deaths 27.5, forecast deaths 25.5, change 7.2%
    """
    # HINTS
    # Witin one agg to: count('date'), avg the past_mean forecast_mean, and max the most recent date
    # avgs = df.agg(...).collect()[0]
    # past_mean = avgs['past_mean']
    # forecast_mean = avgs['forecast_mean']
    # Use resulting values to compute mean_change = forecast_mean / past_mean * 100
    # logger.info(f"Averages for {count} ... ")
    pass


def test_state_forecast(data: DataFrame) -> pd.DataFrame:
    """
    Generates forecast for one state without using a UDF.
    :param data: Spark DataFrame of COVID normalized data as daily totals.
    :return: Pandas DataFrame of Prophet forecast of COVID deaths.
    """
    # Aggregate cases and deaths by day
    states_df = data.groupBy('state', 'date') \
        .agg(F.sum('daily_cases').alias('daily_cases'),
             F.sum('daily_deaths').alias('daily_deaths')) \
        .selectExpr('state', 'date', 'daily_deaths as deaths', 'daily_cases as cases')
    # Test with one state
    df_al = states_df.where("state = 'AL'")
    pdf = df_al.toPandas()
    return state_forecast(pdf)


def test_one_state(local_data=True) -> pd.DataFrame:
    """
    Test forecast function on one state without UDF.
    :return: One state summary
    """
    data = create_dataset(local_data=local_data)
    data = cumulative_to_daily(data)
    pdf = test_state_forecast(data)
    print(pdf.drop(['forecast_fig', 'metrics_fig'], axis=1).to_string())
    return pdf


# Runtime is about 10 minutes
def main(local_data=True, max_state=None, wait=False):
    """
    Main program for processing COVID data to produce death forecasts.
    :param local_data: If True, then process local data files; otherwise, use BigQuery
    :param max_state: If not None, then process states that are alphabetically less than the word in max_state. Used for testing.
    :param wait: If True, don't exit when done processing all states.
    :return: Summary PySpark DataFrame generated by applying state_forecast_udf to the data
    """
    logger.info("Start of covid forecast")
    data = create_dataset(local_data, max_state=max_state)
    data = cumulative_to_daily(data)
    logger.info("Starting forecasting.")
    # BUG even with cache, runs UDF 2x for each group (unless only 1 group)
    # persist/cache needed; otherwise, it reruns UDF forecasts
    df = covid_state_predictions(data).orderBy('forecast_mean').persist(StorageLevel.MEMORY_AND_DISK)
    summarize_state_forecasts(df)
    zip_results(df, directory=forecast_directory)
    df.show(60)
    logger.info("End of covid forecast")
    if wait:
        logger.info("Keeping program alive. You'll need to kill the process/pod.")
        await_keyboard()
    return df

# When done, exec into pod to see results
# kubectl exec -it pods/tsf-driver -- bash
# Copy results out with:
# kubectl cp tsf-driver:forecasts.zip forecasts.zip
