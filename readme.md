# Introduction
This project has three student exercises, intended for students new to PySpark and data analytics.
A data scientist should be able to read any article or blog post about big data and apply the technology to their problem at hand. 
I took interest in the use of [Facebook's Prophet](https://facebook.github.io/prophet/) for time series forecasting, which is described in this post: 
[Fine-Grained Time Series Forecasting at Scale With Facebook Prophet and Apache Spark: Updated for Spark 3](https://databricks.com/blog/2021/04/06/fine-grained-time-series-forecasting-at-scale-with-facebook-prophet-and-apache-spark-updated-for-spark-3.html). 
In a prior project, I demonstrated some data manipulations of public COVID 19 data:
* https://www.kaggle.com/datasets/bigquery/covid19-usafacts
* https://usafacts.org/visualizations/coronavirus-covid-19-spread-map/
* https://console.cloud.google.com/marketplace/product/usafacts-public-data/covid19-us-cases

Naturally, I thought it might be fun to apply the Facebook's Prophet forecasting to the COVID 19 data.
The [article](https://databricks.com/blog/2021/04/06/fine-grained-time-series-forecasting-at-scale-with-facebook-prophet-and-apache-spark-updated-for-spark-3.html) 
as well as a example [notebook](https://nbviewer.org/github/nicolasfauchereau/Auckland_Cycling/blob/master/notebooks/Auckland_cycling_and_weather.ipynb) 
provide a guide to using [Facebook's Prophet](https://facebook.github.io/prophet/). 
However, there's still a few steps to make it work for the [COVID 19 data set](https://console.cloud.google.com/marketplace/product/usafacts-public-data/covid19-us-cases).

This project is intended to illustrate to students the common problem of adapting another project’s modeling techniques. 
It's not a complete professional project. It's simply a student exercise illustrating a common data scientist task.
Be aware that there's a newer version of [Facebook's Prophet](https://facebook.github.io/prophet/), [Neural Prophet](https://ai.facebook.com/blog/neuralprophet-the-neural-evolution-of-facebooks-prophet):
* https://ai.facebook.com/blog/neuralprophet-the-neural-evolution-of-facebooks-prophet
* https://towardsdatascience.com/how-to-use-facebooks-neuralprophet-and-why-it-s-so-powerful-136652d2da8b

## COVID 19 code
The general idea is to use PySpark's UDF to run regular Python code (Prophet) in parallel (via the UDFs).
The software architecture is as follows:
* Read the data using PySpark.
* Partition by state and sort the data within each county.
* Use applyInPandas to apply a UDF to each (state) group of data. 
The UDF is state_forecast_udf, which generates a [Facebook's Prophet](https://facebook.github.io/prophet/) COVID-19 death forecast.

The following are important functions found in the code:
* **normalize_covid**, converts the raw data into a useful table .
* **accumulative_to_daily**, converts the tabular data from cumulative to daily numbers.
* **state_forecast**, generates state covid death forecasts by presenting the target series, 
along with additional regressor time series, to the forecast function, and then saves resulting graphs as a base64 string in a DataFrame.
Later, these strings are converted back to an image and saved.
The input columns are [state, date, deaths, additional regressors...]. Daily cases is the only additional regressor.
To predict deaths, Prophet has deaths as input, along with daily cases for all dates, including those in the forecast period.
To obtain the future daily cases, I use Prophet to predict future cases and use the known and forecast cases as an additional regressor column.
This application of additional regressors is automated. So, one can add additional regressors simply by adding them to the input table.
Potential additional regressors could be weather data (temperature, humidity), vaccine percentage, etc.

## Code to write
Students are to write two functions:
* **covid_state_predictions**, generates state forecasts of COVID deaths. 
It is based on the given function test_state_forecast. So,  students should be able to use the given outline of covid_state_predictions, 
along with the code for test_state_forecast to write covid_state_predictions.
Knowledge required: PySpark groupBy, agg, select or selectExpr, applyInPandas.
```python
# TODO Write this based on test_state_forecast and the use of applyInPandas with state_forecast_udf
#  schema=("state string, date date, past_mean float, forecast_mean float, mean_change float, forecast_fig string, metrics_fig string"))
def covid_state_predictions(data) -> DataFrame:
    """
    Generates state forecasts of COVID deaths.
    For each state, a figure is saved for the forecast and RMSE metric.
    :return: Summary PySpark DataFrame generated by applying state_forecast_udf to the data
    """
    # HINTS
    # First, groupBy state, agg, sum to create a DataFrame having "state", "date", "daily_deaths as deaths", "daily_cases as cases"
    # Then, return the result of applyInPandas state_forecast_udf to the above DataFrame
    # Like Listing 9.11 Split-apply-combing in PySpark in Rioux
    # https://www.manning.com/books/data-analysis-with-python-and-pyspark
    pass

```
* **summarize_state_forecasts**, Logs a 1-line summary of the average past and forecast deaths, and average change. 
The sample output and outline of the function shared provide sufficient information to write this function.
Knowledge required: PySpark agg.
```python
# TODO Write this summary by using PySpark agg function with avg and max over the covid_state_predictions summary df
def summarize_state_forecasts(df):
    """
    Logs a 1-line summary of the average past and forecast deaths, and average change.
    :param df: covid_state_predictions summary of predictions.
    :return: None
    Example output:
    22/04/24 19:34:53 INFO: tsf: Averages for 51 states with last update at 2022-02-23 are:
    past deaths 27.5, forecast deaths 25.5, change 7.2%
    """
    # HINTS
    # Witin one agg to: count('date'), avg the past_mean forecast_mean, and max the most recent date
    # avgs = df.agg(...).collect()[0]
    # past_mean = avgs['past_mean']
    # forecast_mean = avgs['forecast_mean']
    # Use resulting values to compute mean_change = forecast_mean / past_mean * 100
    # logger.info(f"Averages for {count} ... ")
    pass
```

## Problems to solve
There are three problems to be solved in this project:
1. **Generate forecasts from local data**. Complete the code for the two above functions.
2. **Generate forecasts from current Google BigQuery data**. Download Google BigQuery credentials for a project and reference them in the given code. 
Then, run the code against the BigQuery data set, rather than the provided json data files. 
(Change the `local_data` flag to `False`, in the call to `create_dataset`.)
3. **Deploy the project to a Kubernetes cluster**. 
This can be done by running hey Google Cloud Code plugin, which runs the scaffold commands to deploy (locally or remotely). 
Please review the `kube_readme.md` file for deployment to a Kubernetes cluster.

Problem 1 must be solved first. Then, the other two can be independently solved.

## Future work
If you find the project interesting, then two potential improvements are:
* Incoroporate other data as additional regressors.
Potential additional regressors could be weather data (temperature, humidity), vaccine percentage, etc.
* Adapt the code to use [Neural Prophet](https://ai.facebook.com/blog/neuralprophet-the-neural-evolution-of-facebooks-prophet)

## Prophet references
* https://peerj.com/preprints/3190/
* https://facebook.github.io/prophet/
* https://facebook.github.io/prophet/docs/quick_start.html
* https://github.com/facebook/prophet
* https://nbviewer.org/github/nicolasfauchereau/Auckland_Cycling/blob/master/notebooks/Auckland_cycling_and_weather.ipynb

# Demonstration notebooks
There are two notebook to aid in this exploration.
* **local_covid**, illustrates how to run the program on the local data (found in the `data` directory)
* **bigquery_covid**, illustrates how to run the program using the current BigQuery data.

The notebooks have been tested within PyCharm; however, they can be run by importing into Databricks, etc.

# Python modules
See the requirements.txt file, which contains the required modules.
The code depends on [PyStan](https://github.com/stan-dev/pystan) and Prophet. Install the modules as follows:
```shell
conda install pystan==2.19.1.1
conda install prophet
conda install plotly
```

# Download Google BigQuery credentials 

These BigQuery steps only needed when getting data from BigQuery (non-local):
```python
main(local_data=False)
```

To use the Google BigQuery client, you must have a credentials file.
Create and download the json credentials file and place it in the project directory, naming it bq_credentials.json.

See the [9.1.1 Connecting Spark to Google’s BigQuery](https://www.manning.com/books/data-analysis-with-python-and-pyspark)

To obtain the credentials file, scan the instructions from Google:
[Getting started with authentication](https://cloud.google.com/docs/authentication/getting-started#create-service-account-console)

The steps are nicely illustrated in this [Google BigQuery with Python example](https://blog.getcensus.com/how-to-hack-it-extracting-data-from-google-bigquery-with-python-2/),
which is summarized here. (However, you'll want to read the instructions).
1. Google Cloud console, go to IAM & Admin, and select service accounts
   * create service account
   * select both the “BigQuery User”, “Owner”, "User Job" roles
2. On the newly created service acccount, 
   * select actions → manage keys → add a key → create a new key (from the dotted menu at right)
   * create a JSON type key and then save the key credentials file as `bq_credentials.json` in your project
     * Keep the file safe, so someone doesn't use it to access your account.

The downloaded key, as `bq_credentials.json` in your project, will be used to access BigQuery datasets.
If you look inside the file, you'll see the `project_id` that you'll need in your code (covid_tsf.py).
You'll also need to ensure the proper  `bq_credentials_file` path, and  `bq_project_dataset` 
(which is the location of temporary BigQuery tables)

```python
bq_credentials_file = "./bq_credentials.json"
bq_project_id = 'spark8795'
bq_project_dataset = 'spark8795'
```

## Add a dataset for Google BigQuery queries (Optional)

If you use BigQuery query rather than table load to access data, 
then you'll need to have a dataset that's writable in your project.
This project uses table load, so these steps are not necessary. 
They're here in case you want to use queries in an extension to this project.

The BigQuery client needs a dataset to store temporary results of queries. 
The name of the dataset is specified in `bq_project_dataset` in the code (covid_tsf.py).
We must create a  Google BigQuery dataset. To do so:
1. select BigQuery | SQL workspace in the GCP menu (upper left on the [GCP Console](https://console.cloud.google.com)
2. In the Explorer (at the left), select your project's menu (three dots)
  * Create dataset
  * Give it a dateset ID, this is the ID in your code `bq_project_dataset`
  * Choose your location (us-east1)
  * Create dataset button

Now, you can run the BigQuery client, which will read the `bq_credentials_file` and store results to the `bq_project_dataset`.

# Deploying to Kubernetes

Please review the `kube_readme.md` file for deployment to a Kubernetes cluster.

When complete, retreive the results by copying from the pod to the local computer:
```shell
kubectl cp tsf-driver:forecasts.zip forecasts.zip
```

# Bugs, problems
* Docker build will fail (sometimes) when version numbers are specified (for PySpark, numpy).
Google Cloud Code plugin will fail silently with such errors. 
Run the build in the shell to see the build problems:
```shell
docker build . -t k8sparktsf:latest
```

* If there is more than one state, then the UDF called by applyInPandas is called twice. :-( 